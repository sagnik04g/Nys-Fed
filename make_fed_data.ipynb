{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], shape=(11055,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "data_phising_X,data_phising_Y=load_svmlight_file(r'/home/dslab/Sagnik/Avi_ADMM/FedNS (copy)/datasets_clean/phishing')\n",
    "data_phising_X=data_phising_X.toarray()\n",
    "data_phising_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.], shape=(49749,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "data_w8a_X,data_w8a_Y=load_svmlight_file(r'/home/dslab/Sagnik/Avi_ADMM/FedNS (copy)/datasets_clean/w8a')\n",
    "data_w8a_X=data_w8a_X.toarray()\n",
    "data_w8a_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7], shape=(10000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets,transforms\n",
    "\n",
    "transform_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_train=datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_)\n",
    "cifar10_test=datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_)\n",
    "cifar10_train_X=cifar10_train.data\n",
    "cifar10_test_X=cifar10_test.data\n",
    "cifar10_train_Y=np.array([label for _,label in cifar10_train])\n",
    "cifar10_test_Y=np.array([label for _,label in cifar10_test])\n",
    "cifar10_test_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(dataset_X,dataset_Y, num_clients, alpha):\n",
    "    data_indices = np.arange(len(dataset_X))\n",
    "    targets = np.array(dataset_Y)\n",
    "    num_classes = len(np.unique(targets))\n",
    "\n",
    "    # Create Dirichlet distribution\n",
    "    class_distribution = np.random.dirichlet(alpha=[alpha] * num_clients, size=num_classes)\n",
    "\n",
    "    client_data_indices = [[] for _ in range(num_clients)]\n",
    "    for class_idx, class_dist in enumerate(class_distribution):\n",
    "        class_indices = data_indices[targets == class_idx]\n",
    "        np.random.shuffle(class_indices)\n",
    "        split_indices = np.array_split(class_indices, [int(np.round(val)) for val in np.cumsum(class_dist[:-1]) * len(class_indices)])\n",
    "        for client_idx, client_indices in enumerate(split_indices):\n",
    "            client_data_indices[client_idx].extend(client_indices)\n",
    "\n",
    "    return client_data_indices\n",
    "\n",
    "def assign_data_to_clients_niid(train_dataset_X,train_dataset_Y, no_of_clients,alpha):\n",
    "    client_indices = partition_data(train_dataset_X, train_dataset_Y, no_of_clients, alpha)\n",
    "    client_datasets = [(train_dataset_X[indices],train_dataset_Y[indices]) for indices in client_indices]\n",
    "    return client_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(client_dataset):\n",
    " client_data_dict={}\n",
    " user=0\n",
    " for X,y in client_dataset:\n",
    "    if(X.shape[0]!=0):\n",
    "        client_data_dict['user'+str(user)]={'X':X,'Y':y}\n",
    "        user+=1\n",
    " return client_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create non-iid  federated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phishing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_client_datas=assign_data_to_clients_niid(data_phising_X,data_phising_Y,100,0.5)\n",
    "phishing_client_data_train=phishing_client_datas[:70]\n",
    "phishing_client_data_test=phishing_client_datas[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train_dict_phishing=make_json(phishing_client_data_train)\n",
    "client_test_dict_phishing=make_json(phishing_client_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fed_phishing_train_niid.json','w')as f1:\n",
    "    json.dump(client_train_dict_phishing,f1, cls=NumpyEncoder)\n",
    "with open('fed_phishing_test_niid.json','w')as f2:\n",
    "    json.dump(client_test_dict_phishing,f2, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w8a data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w8a_client_datas=assign_data_to_clients_niid(data_w8a_X,data_w8a_Y,100,0.5)\n",
    "w8a_client_data_train=w8a_client_datas[:70]\n",
    "w8a_client_data_test=w8a_client_datas[70:]\n",
    "client_train_dict_w8a=make_json(w8a_client_data_train)\n",
    "client_test_dict_w8a=make_json(w8a_client_data_test)\n",
    "import json\n",
    "with open('fed_w8a_train_niid.json','w')as f1:\n",
    "    json.dump(client_train_dict_w8a,f1, cls=NumpyEncoder)\n",
    "with open('fed_w8a_test_niid.json','w')as f2:\n",
    "    json.dump(client_test_dict_w8a,f2, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_client_datas=assign_data_to_clients_niid(cifar10_train_X,cifar10_train_Y,100,0.5)\n",
    "cifar10_test_client_datas=assign_data_to_clients_niid(cifar10_test_X,cifar10_test_Y,64,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train_dict_cifar10=make_json(cifar10_train_client_datas)\n",
    "client_test_dict_cifar10=make_json(cifar10_test_client_datas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fed_cifar10_train_niid.json','w')as f1:\n",
    "    json.dump(client_train_dict_cifar10,f1, cls=NumpyEncoder)\n",
    "with open('fed_cifar10_test_niid.json','w')as f2:\n",
    "    json.dump(client_test_dict_cifar10,f2, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create iid federated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_data_to_clients_iid(train_dataset_X,train_dataset_Y):\n",
    "    num_samples=int(len(train_dataset_Y)/40)\n",
    "    idx=0\n",
    "    client_indices = []\n",
    "    for ni in range(40):\n",
    "        first=idx\n",
    "        last=first+num_samples\n",
    "        client_indices.append([i for i in range(first,last)])\n",
    "        idx+=num_samples\n",
    "    client_datasets = [(train_dataset_X[indices],train_dataset_Y[indices]) for indices in client_indices]\n",
    "    return client_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datas_iid=assign_data_to_clients_iid(data_phising_X,data_phising_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_client_dataset=client_datas_iid[:]\n",
    "test_client_dataset=client_datas_iid[26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_client_dataset=client_datas[:]\n",
    "test_client_dataset=client_datas[26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n",
      "276\n"
     ]
    }
   ],
   "source": [
    "sum_n_samples=0\n",
    "for _, client_data_y in  train_client_dataset:\n",
    "    print(len(client_data_y))\n",
    "# print(sum_n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train_dict=make_json(train_client_dataset)\n",
    "client_test_dict=make_json(test_client_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fed_phishing_train_iid.json','w')as f1:\n",
    "    json.dump(client_train_dict,f1, cls=NumpyEncoder)\n",
    "with open('fed_phishing_test_iid.json','w')as f2:\n",
    "    json.dump(client_test_dict,f2, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "18\n",
      "388\n",
      "1\n",
      "804\n",
      "624\n",
      "253\n",
      "17\n",
      "311\n",
      "12\n",
      "9\n",
      "39\n",
      "1\n",
      "5\n",
      "1\n",
      "381\n",
      "343\n",
      "2659\n",
      "64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('fed_phishing_train.json','r') as r1:\n",
    "    data_json=json.load(r1)\n",
    "for user,data in data_json.items():\n",
    "    print(len(data['Y']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dslab'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m update_clients\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_datas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m update_clients\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:956\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "update_clients=np.random.choice(client_datas, 10, replace = False)\n",
    "update_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "json_path= r\"/home/dslab/Sagnik/datasets/fl_data/imbalanced/train/mnist_data_train_invdpp_s=5.pickle\"\n",
    "final_data_dict={}\n",
    "t=transforms.Compose(\n",
    "        [transforms.Pad(18),\n",
    "         transforms.Resize((64, 64)), transforms.ToTensor()])\n",
    "with open(json_path, 'rb') as f:\n",
    "    tmp_data_dict = pickle.load(f)\n",
    "    \n",
    "    for user,data in tmp_data_dict.items():\n",
    "        if len(data['y']) < 1:\n",
    "         continue\n",
    "\n",
    "        ys_final = data['y']\n",
    "        xs=[]\n",
    "        for x in data['x']:\n",
    "          print(x.shape)\n",
    "          x_img=np.array(x).reshape(28,28)\n",
    "          x_img=Image.fromarray(np.uint8(x_img))\n",
    "          x=t(x_img)\n",
    "          xs.append(x)\n",
    "        xs_final=torch.stack(xs).float()\n",
    "        ys_final=torch.as_tensor(ys_final).long()\n",
    "        final_data_dict[user]={'x' : xs_final,'y' : ys_final}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358, 784)\n"
     ]
    }
   ],
   "source": [
    "print(tmp_data_dict['user_0']['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nysn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
